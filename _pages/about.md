---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

My research interest includes machine learning, generative models, computer grahics, and computer vision... I am currently a Postdoc in KAUST, working with [Peter Wonka](https://peterwonka.net). Before that, I obtained the BSc and MSc from Xi'an Jiaotong University.


# üî• News
<!-- - *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
- *2025.04*: I will be attending ICLR 2025 in Singapore.


# üìö Preprints

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv 2025</div><img src='images/arxiv-lari.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LaRI: Layered Ray Intersections for Single-view 3D Geometric Reasoning
](https://arxiv.org/abs/2504.18424)

Rui Li, **Biao Zhang**, Zhenyu Li, Federico Tombari, Peter Wonka

[**Project**](https://ruili3.github.io/lari), [**Code**](https://ruili3.github.io/lari)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv 2025</div><img src='images/arxiv-iflame.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[iFlame: Interleaving Full and Linear Attention for Efficient Mesh Generation
](https://arxiv.org/abs/2503.16653)

Hanxiao Wang, **Biao Zhang**üìß, Weize Quan, Dong-Ming Yan, Peter Wonka

[**Project**](https://hanxiaowang00.github.io/iFlame/), [**Code**](https://github.com/hanxiaowang00/iFlame)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv 2024</div><img src='images/arxiv-geomdist.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Geometry Distributions
](https://arxiv.org/abs/2411.16076)

**Biao Zhang**, Jing Ren, Peter Wonka

[**Project**](https://1zb.github.io/GeomDist/), [**Code**](https://github.com/1zb/GeomDist)

</div>
</div>



# üìù Publications

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/iclr2025.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LaGeM: A Large Geometry Model for 3D Representation Learning and Diffusion
](https://arxiv.org/abs/2410.01295) (<span style="color:blue">*ICLR 2025*</span>)

**Biao Zhang**, Peter Wonka

[**Project**](https://1zb.github.io/LaGeM), [**Code**](https://github.com/1zb/LaGeM), [**OpenReview**](https://openreview.net/forum?id=72OSO38a2z)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/neurips2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Vivid-ZOO: Multi-View Video Generation with Diffusion Model
](https://arxiv.org/pdf/2406.08659v1) (<span style="color:blue">*NeurIPS 2024*</span>)

Bing Li\*, Cheng Zheng\*, Wenxuan Zhu\*, Jinjie Mai, **Biao Zhang**, Peter Wonka, Bernard Ghanem

[**Project**](https://hi-zhengcheng.github.io/vividzoo/), [**Code**](https://github.com/hi-zhengcheng/vividzoo)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/cvpr2024-func.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Functional Diffusion
](https://arxiv.org/abs/2311.15435) (<span style="color:blue">*CVPR 2024*</span>)

**Biao Zhang**, Peter Wonka

[**Project**](https://1zb.github.io/functional-diffusion/), [**Code**](https://1zb.github.io/functional-diffusion/)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/cvpr2024-motion.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape Reconstruction and Tracking
](https://arxiv.org/abs/2401.06614) (<span style="color:blue">*CVPR 2024*</span>)

Wei Cao\*, Chang Luo\*, **Biao Zhang**, Matthias Niessner, Jiapeng Tang

[**Project**](https://vveicao.github.io/projects/Motion2VecSets/), [**Code**](https://vveicao.github.io/projects/Motion2VecSets/)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SIGGRAPH 2023 (ToG)</div><img src='images/sg2023.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[3DShape2VecSet: A 3d shape representation for neural fields and generative diffusion models](https://arxiv.org/abs/2205.13914) (<span style="color:blue">*SIGGRAPH 2023*</span>)

**Biao Zhang**, Peter Wonka

[**Project**](https://1zb.github.io/3DShape2VecSet), [**Code** (Legacy)](https://github.com/1zb/3DShape2VecSet/),  [**Code** (Latest)](https://github.com/1zb/VecSetX/), [**ToG**](https://dl.acm.org/doi/10.1145/3592442)

- The work has been integrated into some large 3D foundation models, such as Tripo, CLAY, and Hunyuan3D.
- I gave a talk about this work in [Towards 3D Foundation Models: Progress and Prospects](https://3dfm.github.io)
- Online talk [Meshy](https://www.bilibili.com/video/BV1WP411d777/), [GAMES Webinar 1](https://www.bilibili.com/video/BV1BH4y1X7zW/), [GAMES Webinar 2](https://www.bilibili.com/video/BV1CvsQeMEDd/)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/neurips2022.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[3DILG: Irregular Latent Grids for 3D Generative Modeling](https://arxiv.org/abs/2205.13914) (<span style="color:blue">*NeurIPS 2022*</span>)

**Biao Zhang**, Peter Wonka

[**Project**](https://1zb.github.io/3DILG/), [**Code**](https://github.com/1zb/3DILG/)


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2022</div><img src='images/iclr2022.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization](https://arxiv.org/abs/2010.08276) (<span style="color:blue">*ICLR 2022*</span>)

**Biao Zhang**, Peter Wonka

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Siggraph Asia 2021 (ToG)</div><img src='images/sga2021.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Intuitive and Efficient Roof Modeling for Reconstruction and Synthesis](https://arxiv.org/abs/2109.07683) (<span style="color:blue">*SIGGRAPH Asia 2021*</span>)

Jing Ren, **Biao Zhang**, Bojian Wu, Jianqiang Huang, Lubin Fan, Maks Ovsjanikov, Peter Wonka

[**Code**](https://github.com/llorz/SGA21_roofOptimization)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2021</div><img src='images/cvpr2021.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Point cloud instance segmentation using probabilistic embeddings](https://arxiv.org/abs/1912.00145) (<span style="color:blue">*CVPR 2021*</span>)

**Biao Zhang**, Peter Wonka

<!-- [**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
- Featured in [CVPR 2021 ScanNet workshop](http://www.scan-net.org/cvpr2021workshop/).
</div>
</div>

<!-- - [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->

<!-- # üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìñ Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->

# üë™ Collaborators
People I have worked with (we have at least submitted one paper):
- [Wei Cao](https://vveicao.github.io) (TU Munich)
- [Bing Li](https://scholar.google.com/citations?user=xBiftlUAAAAJ) (KAUST)
- [Rui Li](https://ruili3.github.io) (KAUST)
- [Matthias Niessner](https://www.niessnerlab.org) (TU Munich)
- [Jing Ren](https://ren-jing.com) (ETH Zurich)
- [Jiapeng Tang](https://tangjiapeng.github.io) (TU Munich)
- [Hanxiao Wang](https://scholar.google.com/citations?user=rPvC5AkAAAAJ) (CASIA)
- [Peter Wonka](https://peterwonka.net) (KAUST)