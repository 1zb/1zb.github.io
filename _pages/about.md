---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

My research interest includes machine learning, generative models, computer grahics, and computer vision... I am an incoming Professor of Xi'an Jiaotong University. I obtained PhD Degree in KAUST, working with [Peter Wonka](https://peterwonka.net). Before that, I obtained the BSc and MSc from Xi'an Jiaotong University.

<span style="color:red">*Openings: I am always looking for self-motivated students to work with me. If you are interested in my research, feel free to contact me with your CV and a brief research statement.*
</span>

# üî• News
<!-- - *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
- *2025.12*: I will be attending SIGGRAPH Asia 2025 in Hong Kong (China) from Dec 14 to Dec 19.
- *2025.10*: I will be attending ICCV 2025 in Hawaii (USA) from Oct 18 to Oct 24.
- *2025.06*: I will be co-organizing the workshop [Ind3D: Enforcing Inductive Bias in 3D Generation](https://ind3dworkshop.github.io/cvpr2025/) on June 12, 2025, during CVPR. Feel free to drop by to chat.
- *2025.06*: I will be assisting Professor Wonka with his talk at the [2nd Workshop on Urban Scene Modeling](https://usm3d.github.io/) on June 11, 2025, during CVPR. Feel free to drop by to chat.
- *2025.04*: I will be attending ICLR 2025 in Singapore.




# üìù Publications
<div id="publications-wrapper">

<div id="filter-container"></div>


<div class='paper-box' data-tags="4D Generation, Human Avatars"><div class='paper-box-image'><div><div class="badge">CVPR 2026</div><img src='images/cvpr-animate.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Human Geometry Distribution for 3D Animation Generation](https://arxiv.org/abs/2512.07459)

Xiangjun Tang, **Biao Zhang**, Peter Wonka

<!-- [**Project**](https://github.com/xhanxu/LumiX) -->
<span style="color:blue">*CVPR 2026*</span>

</div>
</div>

<div class='paper-box' data-tags="Image Generation"><div class='paper-box-image'><div><div class="badge">CVPR 2026</div><img src='images/cvpr-lumix.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LumiX: Structured and Coherent Text-to-Intrinsic Generation](https://arxiv.org/abs/2512.02781)

Xu Han, **Biao Zhang**, Xiangjun Tang, Xianzhi Li, Peter Wonka

[**Project**](https://github.com/xhanxu/LumiX)

<span style="color:blue">*CVPR 2026*</span>

</div>
</div>

<div class='paper-box' data-tags="3D Detection"><div class='paper-box-image'><div><div class="badge">CVPR 2026</div><img src='images/cvpr-posegam.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning](https://arxiv.org/abs/2512.10840)

Jianqi Chen, **Biao Zhang**, Xiangjun Tang, Peter Wonka

[**Project**](https://windvchen.github.io/PoseGAM/)

<span style="color:blue">*CVPR 2026*</span>

</div>
</div>

<div class='paper-box' data-tags="3D Generation, Human Avatars"><div class='paper-box-image'><div><div class="badge">ICLR 2026</div><img src='images/iclr-human.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Generative Human Geometry Distribution
](https://arxiv.org/abs/2503.01448)

Xiangjun Tang, **Biao Zhang**üìß, Peter Wonka

<!-- [**Project**](https://ruili3.github.io/lari), [**Code**](https://ruili3.github.io/lari) -->
<span style="color:blue">*ICLR 2026*</span> <span style="color:red"> (Oral)</span>

</div>
</div>


<div class='paper-box' data-tags="3D Generation, Brep"><div class='paper-box-image'><div><div class="badge">SIGGRAPH Asia 2025</div><img src='images/sga2025-brep.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[BrepGPT: Autoregressive B-rep Generation with Voronoi Half-Patch](https://arxiv.org/abs/2511.22171)


Pu Li, Wenhao Zhang, Weize Quan, **Biao Zhang**, Peter Wonka, Dongming Yan


<span style="color:blue">*SIGGRAPH Asia 2025 (ToG)*</span>
</div>
</div>



<div class='paper-box' data-tags="3D Generation"><div class='paper-box-image'><div><div class="badge">SIGGRAPH Asia 2025</div><img src='images/sga2025-tree.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Autoregressive Generation of Static and Growing Trees
](https://arxiv.org/abs/2502.04762) 

Hanxiao Wang, **Biao Zhang**, Jonathan Klein, Dominik L. Michels, Dongming Yan, Peter Wonka

<!-- [**Project**](https://1zb.github.io/LaGeM), [**Code**](https://github.com/1zb/LaGeM), [**OpenReview**](https://openreview.net/forum?id=72OSO38a2z) -->

<span style="color:blue">*SIGGRAPH Asia 2025 Conference*</span>
</div>
</div>


<div class='paper-box' data-tags="First Author, 3D Representation"><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/iccv-geomdist.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Geometry Distributions
](https://arxiv.org/abs/2411.16076) 

**Biao Zhang**, Jing Ren, Peter Wonka

[**Project**](https://1zb.github.io/GeomDist/), [**Code**](https://github.com/1zb/GeomDist), [**ICCV**](https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Geometry_Distributions_ICCV_2025_paper.html)

<span style="color:blue">*ICCV 2025*</span> <span style="color:red"> (Highlight)</span>

</div>
</div>

<div class='paper-box' data-tags="4D Generation"><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/iccv-v2m4.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[V2M4: 4D Mesh Animation Reconstruction from a Single Monocular Video
](https://arxiv.org/abs/2503.09631) 

Jianqi Chen, **Biao Zhang**, Xiangjun Tang, Peter Wonka

[**Project**](https://windvchen.github.io/V2M4/)

<span style="color:blue">*ICCV 2025*</span>

</div>
</div>

<div class='paper-box' data-tags="3D Generation, Texturing"><div class='paper-box-image'><div><div class="badge">SIGGRAPH 2025</div><img src='images/sg2025-matclip.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MatCLIP: Light-and Shape-Insensitive Assignment of PBR Material Models
](https://arxiv.org/abs/2501.15981) 

Michael Birsak, John Femiani, **Biao Zhang**, Peter Wonka

<!-- [**Project**](https://1zb.github.io/LaGeM), [**Code**](https://github.com/1zb/LaGeM), [**OpenReview**](https://openreview.net/forum?id=72OSO38a2z) -->

<span style="color:blue">*SIGGRAPH 2025 Conference*</span>
</div>
</div>

<div class='paper-box' data-tags="First Author, 3D Generation, 3D Representation"><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/iclr2025.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LaGeM: A Large Geometry Model for 3D Representation Learning and Diffusion
](https://arxiv.org/abs/2410.01295) 

**Biao Zhang**, Peter Wonka

[**Project**](https://1zb.github.io/LaGeM), [**Code**](https://github.com/1zb/LaGeM), [**OpenReview**](https://openreview.net/forum?id=72OSO38a2z)

<span style="color:blue">*ICLR 2025*</span>
</div>
</div>

<div class='paper-box' data-tags="4D Generation"><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/neurips2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Vivid-ZOO: Multi-View Video Generation with Diffusion Model
](https://arxiv.org/pdf/2406.08659v1) 

Bing Li\*, Cheng Zheng\*, Wenxuan Zhu\*, Jinjie Mai, **Biao Zhang**, Peter Wonka, Bernard Ghanem

[**Project**](https://hi-zhengcheng.github.io/vividzoo/), [**Code**](https://github.com/hi-zhengcheng/vividzoo)

<span style="color:blue">*NeurIPS 2024*</span>
</div>
</div>

<div class='paper-box' data-tags="First Author, 3D Generation"><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/cvpr2024-func.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Functional Diffusion
](https://arxiv.org/abs/2311.15435) 

**Biao Zhang**, Peter Wonka

[**Project**](https://1zb.github.io/functional-diffusion/), [**Code**](https://1zb.github.io/functional-diffusion/)

<span style="color:blue">*CVPR 2024*</span>
</div>
</div>

<div class='paper-box' data-tags="4D Generation"><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/cvpr2024-motion.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape Reconstruction and Tracking
](https://arxiv.org/abs/2401.06614) 

Wei Cao\*, Chang Luo\*, **Biao Zhang**, Matthias Niessner, Jiapeng Tang

[**Project**](https://vveicao.github.io/projects/Motion2VecSets/), [**Code**](https://vveicao.github.io/projects/Motion2VecSets/)

<span style="color:blue">*CVPR 2024*</span>
</div>
</div>

<div class='paper-box' data-tags="First Author, 3D Generation, 3D Representation"><div class='paper-box-image'><div><div class="badge">SIGGRAPH 2023 (ToG)</div><img src='images/sg2023.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[3DShape2VecSet: A 3d shape representation for neural fields and generative diffusion models](https://arxiv.org/abs/2205.13914) 

**Biao Zhang**, Peter Wonka

[**Project**](https://1zb.github.io/3DShape2VecSet), [**Code** (Legacy)](https://github.com/1zb/3DShape2VecSet/),  [**Code** (Latest)](https://github.com/1zb/VecSetX/), [**ToG**](https://dl.acm.org/doi/10.1145/3592442)

- The work has been integrated into some large 3D foundation models, such as VAST/Tripo, CLAY, and Hunyuan3D.
- I gave a talk about this work in [Towards 3D Foundation Models: Progress and Prospects](https://3dfm.github.io)
- Online talk [Meshy](https://www.bilibili.com/video/BV1WP411d777/), [GAMES Webinar 1](https://www.bilibili.com/video/BV1BH4y1X7zW/), [GAMES Webinar 2](https://www.bilibili.com/video/BV1CvsQeMEDd/)

<span style="color:blue">*SIGGRAPH 2023 (ToG)*</span>
</div>
</div>

<div class='paper-box' data-tags="First Author, 3D Generation, 3D Representation"><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/neurips2022.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[3DILG: Irregular Latent Grids for 3D Generative Modeling](https://arxiv.org/abs/2205.13914) 

**Biao Zhang**, Peter Wonka

[**Project**](https://1zb.github.io/3DILG/), [**Code**](https://github.com/1zb/3DILG/)

<span style="color:blue">*NeurIPS 2022*</span>
</div>
</div>

<div class='paper-box' data-tags="First Author, 3D Representation"><div class='paper-box-image'><div><div class="badge">ICLR 2022</div><img src='images/iclr2022.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization](https://arxiv.org/abs/2010.08276) (<span style="color:blue">*ICLR 2022*</span>)

**Biao Zhang**, Peter Wonka

</div>
</div>

<div class='paper-box' data-tags="3D Generation"><div class='paper-box-image'><div><div class="badge">Siggraph Asia 2021 (ToG)</div><img src='images/sga2021.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Intuitive and Efficient Roof Modeling for Reconstruction and Synthesis](https://arxiv.org/abs/2109.07683)

Jing Ren, **Biao Zhang**, Bojian Wu, Jianqiang Huang, Lubin Fan, Maks Ovsjanikov, Peter Wonka

[**Code**](https://github.com/llorz/SGA21_roofOptimization)

 <span style="color:blue">*SIGGRAPH Asia 2021 (ToG)*</span>
</div>
</div>

<div class='paper-box' data-tags="First Author, 3D Detection"><div class='paper-box-image'><div><div class="badge">CVPR 2021</div><img src='images/cvpr2021.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Point cloud instance segmentation using probabilistic embeddings](https://arxiv.org/abs/1912.00145) 

**Biao Zhang**, Peter Wonka

<!-- [**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
- Featured in [CVPR 2021 ScanNet workshop](http://www.scan-net.org/cvpr2021workshop/).

<span style="color:blue">*CVPR 2021*</span>
</div>
</div>

<!-- - [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->

<!-- # üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìñ Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->
</div>

# üìö Preprints



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv 2025</div><img src='images/arxiv-lari.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LaRI: Layered Ray Intersections for Single-view 3D Geometric Reasoning
](https://arxiv.org/abs/2504.18424)

Rui Li, **Biao Zhang**, Zhenyu Li, Federico Tombari, Peter Wonka

[**Project**](https://ruili3.github.io/lari), [**Code**](https://ruili3.github.io/lari)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv 2025</div><img src='images/arxiv-iflame.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[iFlame: Interleaving Full and Linear Attention for Efficient Mesh Generation
](https://arxiv.org/abs/2503.16653)

Hanxiao Wang, **Biao Zhang**üìß, Weize Quan, Dong-Ming Yan, Peter Wonka

[**Project**](https://hanxiaowang00.github.io/iFlame/), [**Code**](https://github.com/hanxiaowang00/iFlame)

</div>
</div>

# üìù Notes
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Notes</div><img src='images/notes-muon-2025.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Notes on Muon](assets/pdf/muon_notes.pdf) 

**Biao Zhang**

<!-- <span style="color:blue">*Notes*</span> -->
</div>
</div>

# üë™ Collaborators
People I have worked with (we have at least submitted one paper):
- [Wei Cao](https://vveicao.github.io) (TU Munich)
- [Bing Li](https://scholar.google.com/citations?user=xBiftlUAAAAJ) (KAUST)
- [Rui Li](https://ruili3.github.io) (KAUST)
- [Matthias Niessner](https://www.niessnerlab.org) (TU Munich)
- [Jing Ren](https://ren-jing.com) (ETH Zurich)
- [Jiapeng Tang](https://tangjiapeng.github.io) (TU Munich)
- [Hanxiao Wang](https://scholar.google.com/citations?user=rPvC5AkAAAAJ) (CASIA)
- [Peter Wonka](https://peterwonka.net) (KAUST)




<script>
document.addEventListener('DOMContentLoaded', function() {
  const wrapper = document.getElementById('publications-wrapper');
  if (!wrapper) return;

  const filterContainer = document.getElementById('filter-container');
  const paperBoxes = wrapper.querySelectorAll('.paper-box');
  
  let tagCounts = {}; 
  let activeTags = new Set();

  // ÂàùÂßãÂåñÔºöÁîüÊàêÊ†áÁ≠æÂπ∂ÁªüËÆ°Êï∞Èáè
  paperBoxes.forEach(box => {
    const tagsAttribute = box.getAttribute('data-tags');
    if (tagsAttribute) {
      const tagsList = tagsAttribute.split(',').map(t => t.trim()).filter(t => t);
      
      // --- ÊèíÂÖ•Ê†áÁ≠æÂà∞ Links ‰∏äÊñπ ---
      const textContainer = box.querySelector('.paper-box-text');
      const linksContainer = box.querySelector('.links');
      
      if (textContainer && !textContainer.querySelector('.badge-container')) {
        const badgeContainer = document.createElement('div');
        badgeContainer.className = 'badge-container';
        
        tagsList.forEach(tag => {
          const badge = document.createElement('span');
          badge.className = 'inner-tag-badge';
          badge.textContent = tag;
          badgeContainer.appendChild(badge);
        });
        
        if (linksContainer) {
          textContainer.insertBefore(badgeContainer, linksContainer);
        } else {
          textContainer.appendChild(badgeContainer);
        }
      }
      // ---------------------------

      tagsList.forEach(tag => {
        tagCounts[tag] = (tagCounts[tag] || 0) + 1;
      });
    }
  });

  // ÁîüÊàêÈ°∂ÈÉ®ËøáÊª§ÊåâÈíÆ
  const sortedTags = Object.keys(tagCounts).sort();
  if (filterContainer) {
    filterContainer.innerHTML = ''; 
    sortedTags.forEach(tag => {
      const btn = document.createElement('button');
      btn.className = 'filter-btn';
      btn.textContent = `${tag} (${tagCounts[tag]})`;
      
      btn.onclick = () => {
        if (activeTags.has(tag)) {
          activeTags.delete(tag);
          btn.classList.remove('active');
        } else {
          activeTags.add(tag);
          btn.classList.add('active');
        }
        filterPapers(); // ÁÇπÂáªÂêéËß¶ÂèëËøáÊª§ÂíåÈ´ò‰∫ÆÊõ¥Êñ∞
      };
      
      filterContainer.appendChild(btn);
    });
  }

  // üî• Ê†∏ÂøÉÈÄªËæëÊõ¥Êñ∞ÔºöËøáÊª§ËÆ∫Êñá + È´ò‰∫ÆÊ†áÁ≠æ
  function filterPapers() {
    paperBoxes.forEach(box => {
      // 1. Â§ÑÁêÜÂç°ÁâáÊòæÁ§∫/ÈöêËóè
      const boxTagsString = box.getAttribute('data-tags');
      const boxTags = boxTagsString ? boxTagsString.split(',').map(t => t.trim()) : [];
      
      let isVisible = true;
      if (activeTags.size > 0) {
        if (boxTags.length === 0) {
          isVisible = false;
        } else {
          // ÂøÖÈ°ªÂåÖÂê´ÊâÄÊúâÈÄâ‰∏≠ÁöÑÊ†áÁ≠æ (AND ÈÄªËæë)
          isVisible = Array.from(activeTags).every(activeTag => boxTags.includes(activeTag));
        }
      }

      if (isVisible) {
        box.classList.remove('hidden');
      } else {
        box.classList.add('hidden');
      }

      // 2. üî• Â§ÑÁêÜÂÜÖÈÉ®Ê†áÁ≠æÁöÑÈ´ò‰∫Æ (Âç≥‰æøÂç°ÁâáÈöêËóè‰∫ÜÔºåÈÄªËæë‰∏ä‰πüÊõ¥Êñ∞‰∏Ä‰∏ãÔºåÊ≤°ÂùèÂ§Ñ)
      const innerBadges = box.querySelectorAll('.inner-tag-badge');
      innerBadges.forEach(badge => {
        // Â¶ÇÊûúËøô‰∏™Â∞èÊ†áÁ≠æÁöÑÊñáÂ≠óÔºåÂ≠òÂú®‰∫é activeTags (È°∂ÈÉ®ÈÄâ‰∏≠ÁöÑÈõÜÂêà) ‰∏≠ÔºåÂ∞±ÂèòËâ≤
        if (activeTags.has(badge.textContent)) {
          badge.classList.add('active');
        } else {
          badge.classList.remove('active');
        }
      });
    });
  }
});
</script>